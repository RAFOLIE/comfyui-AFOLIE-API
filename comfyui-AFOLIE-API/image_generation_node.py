"""
é«˜çº§å›¾åƒç”ŸæˆèŠ‚ç‚¹
æ”¯æŒ Gemini å’Œ OpenAI åŒåè®®
æ‰¹é‡ç”Ÿæˆã€å¤šå›¾å‚è€ƒã€æ™ºèƒ½å‚æ•°å¤„ç†
"""

from __future__ import annotations

import base64
import json
import time
import os
import sys
import concurrent.futures
from typing import List, Dict, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

import torch
import requests

MODULE_DIR = os.path.dirname(os.path.abspath(__file__))
if MODULE_DIR not in sys.path:
    sys.path.insert(0, MODULE_DIR)

try:
    from server import PromptServer
except ImportError:
    class _DummyPromptServer:
        instance = None
    PromptServer = _DummyPromptServer()

import comfy.utils
import comfy.model_management

from nebula_logger import logger
from nebula_config_manager import ConfigManager
from nebula_image_codec import ImageCodec, ErrorCanvas


class GeminiApiClient:
    """Gemini åè®® API å®¢æˆ·ç«¯"""

    def __init__(self, config_manager: ConfigManager, logger_instance=logger, interrupt_checker=None):
        self.config_manager = config_manager
        self.logger = logger_instance
        self.interrupt_checker = interrupt_checker

    def _build_headers(self, api_key: str) -> Dict[str, str]:
        return {
            "Accept": "application/json",
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}",
        }

    def _ensure_not_interrupted(self):
        if self.interrupt_checker is not None:
            self.interrupt_checker()

    def create_request_data(
        self,
        model: str,
        prompt: str,
        size: Optional[str] = None,
        quality: Optional[str] = None,
        n: int = 1,
        input_images_b64: Optional[List[str]] = None,
        top_p: Optional[float] = None,
        seed: Optional[int] = None,
        image_size: Optional[str] = None,
        **extra_params
    ) -> Dict[str, Any]:
        """åˆ›å»º Gemini è¯·æ±‚æ ¼å¼"""
        request_body: Dict[str, Any] = {
            "model": model,
            "n": n,
            "response_format": "b64_json",
        }

        # å¤„ç†æç¤ºè¯å’Œå‚è€ƒå›¾åƒ
        if input_images_b64:
            parts = []
            for img_b64 in input_images_b64:
                parts.append({"image": f"data:image/png;base64,{img_b64}"})
            if prompt:
                parts.append({"text": prompt})
            request_body["contents"] = [{
                "role": "user",
                "parts": parts
            }]
        else:
            request_body["prompt"] = prompt

        # å®½é«˜æ¯” - ä½¿ç”¨ size å‚æ•°
        if size and size != "Auto":
            request_body["size"] = size
        
        # è´¨é‡ - æ˜ å°„åˆ° image_sizeï¼ˆåˆ†è¾¨ç‡ï¼‰
        # quality: "high"/"2K" -> image_size: "2K"
        # quality: "standard"/"medium"/"low"/"auto"/"1K" -> image_size: "1K"
        if quality:
            if quality in ["high", "2K"]:
                request_body["image_size"] = "2K"
            elif quality in ["standard", "medium", "low", "auto", "1K"]:
                request_body["image_size"] = "1K"
        
        # ç›´æ¥ä½¿ç”¨ image_size å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        if image_size and image_size != "æ— ":
            request_body["image_size"] = image_size

        # å¯é€‰å‚æ•°
        if top_p is not None:
            request_body["top_p"] = top_p
        if seed is not None and seed >= 0:
            request_body["seed"] = seed

        # é¢å¤–å‚æ•°
        for key, value in extra_params.items():
            if value is not None:
                request_body[key] = value

        return request_body

    def send_request(
        self,
        api_key: str,
        request_data: Dict[str, Any],
        api_base_url: str,
        timeout: Optional[Tuple[float, float]] = None,
        bypass_proxy: bool = False,
        verify_ssl: bool = True,
    ) -> Dict[str, Any]:
        """å‘é€ API è¯·æ±‚"""
        sanitized_key = self.config_manager.sanitize_api_key(api_key)
        if not sanitized_key:
            raise ValueError("API Key æ— æ•ˆ")

        url = f"{api_base_url.rstrip('/')}/images/generations"

        session = requests.Session()
        if bypass_proxy:
            session.trust_env = False
            session.proxies = {}

        headers = self._build_headers(sanitized_key)
        payload = json.dumps(request_data, ensure_ascii=False).encode("utf-8")

        if timeout is None:
            timeout = (15, 420)

        try:
            self._ensure_not_interrupted()

            # ä¸­æ–­æ£€æŸ¥å¾ªç¯
            done_event = threading.Event()
            resp_holder = {}
            exc_holder = {}

            def _do_request():
                try:
                    resp_holder["resp"] = session.post(
                        url,
                        data=payload,
                        headers=headers,
                        timeout=timeout,
                        verify=verify_ssl,
                    )
                except BaseException as exc:
                    exc_holder["exc"] = exc
                finally:
                    done_event.set()

            thread = threading.Thread(target=_do_request, daemon=True)
            thread.start()

            poll_interval = 0.25
            while not done_event.wait(timeout=poll_interval):
                self._ensure_not_interrupted()
            self._ensure_not_interrupted()

            if "exc" in exc_holder:
                raise exc_holder["exc"]

            resp = resp_holder.get("resp")
            if resp is None:
                raise RuntimeError("è¯·æ±‚è¢«ä¸­æ–­")

            resp.raise_for_status()
            return resp.json()

        except requests.HTTPError as e:
            error_msg = f"HTTP é”™è¯¯: {e.response.status_code}"
            try:
                error_text = e.response.text[:500]
                error_msg += f"\nå“åº”å†…å®¹: {error_text}"
                
                # å°è¯•è§£æ JSON é”™è¯¯
                try:
                    error_json = e.response.json()
                    if 'error' in error_json:
                        error_msg += f"\né”™è¯¯è¯¦æƒ…: {error_json['error']}"
                except:
                    pass
            except Exception as parse_error:
                error_msg += f"\næ— æ³•è§£æå“åº”: {parse_error}"
            raise RuntimeError(error_msg)
            
        except requests.RequestException as e:
            error_msg = f"è¯·æ±‚å¤±è´¥: {type(e).__name__} - {str(e)}"
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_text = e.response.text[:500]
                    error_msg += f"\nå“åº”å†…å®¹: {error_text}"
                except:
                    pass
            raise RuntimeError(error_msg)


class OpenAIApiClient:
    """OpenAI åè®® API å®¢æˆ·ç«¯"""

    def __init__(self, config_manager: ConfigManager, logger_instance=logger, interrupt_checker=None):
        self.config_manager = config_manager
        self.logger = logger_instance
        self.interrupt_checker = interrupt_checker

    def _build_headers(self, api_key: str) -> Dict[str, str]:
        return {
            "Accept": "application/json",
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}",
        }

    def _ensure_not_interrupted(self):
        if self.interrupt_checker is not None:
            self.interrupt_checker()

    def create_request_data(
        self,
        model: str,
        prompt: str,
        size: Optional[str] = None,
        quality: Optional[str] = None,
        n: int = 1,
        response_format: str = "b64_json",
        input_images_b64: Optional[List[str]] = None,
        top_p: Optional[float] = None,
        seed: Optional[int] = None,
        **extra_params
    ) -> Dict[str, Any]:
        """åˆ›å»º OpenAI è¯·æ±‚æ ¼å¼"""
        request_body: Dict[str, Any] = {
            "model": model,
            "prompt": prompt,
            "n": n,
            "response_format": response_format,
        }

        if size:
            request_body["size"] = size
        if quality:
            request_body["quality"] = quality

        # å¯é€‰å‚æ•°
        if top_p is not None:
            request_body["top_p"] = top_p
        if seed is not None and seed >= 0:
            request_body["seed"] = seed

        # é¢å¤–å‚æ•°
        for key, value in extra_params.items():
            if value is not None:
                request_body[key] = value

        return request_body

    def send_request(
        self,
        api_key: str,
        request_data: Dict[str, Any],
        api_base_url: str,
        timeout: Optional[Tuple[float, float]] = None,
        bypass_proxy: bool = False,
        verify_ssl: bool = True,
    ) -> Dict[str, Any]:
        """å‘é€ API è¯·æ±‚"""
        sanitized_key = self.config_manager.sanitize_api_key(api_key)
        if not sanitized_key:
            raise ValueError("API Key æ— æ•ˆ")

        url = f"{api_base_url.rstrip('/')}/images/generations"

        session = requests.Session()
        if bypass_proxy:
            session.trust_env = False
            session.proxies = {}

        headers = self._build_headers(sanitized_key)
        payload = json.dumps(request_data, ensure_ascii=False).encode("utf-8")

        if timeout is None:
            timeout = (15, 420)

        try:
            self._ensure_not_interrupted()

            done_event = threading.Event()
            resp_holder = {}
            exc_holder = {}

            def _do_request():
                try:
                    resp_holder["resp"] = session.post(
                        url,
                        data=payload,
                        headers=headers,
                        timeout=timeout,
                        verify=verify_ssl,
                    )
                except BaseException as exc:
                    exc_holder["exc"] = exc
                finally:
                    done_event.set()

            thread = threading.Thread(target=_do_request, daemon=True)
            thread.start()

            poll_interval = 0.25
            while not done_event.wait(timeout=poll_interval):
                self._ensure_not_interrupted()
            self._ensure_not_interrupted()

            if "exc" in exc_holder:
                raise exc_holder["exc"]

            resp = resp_holder.get("resp")
            if resp is None:
                raise RuntimeError("è¯·æ±‚è¢«ä¸­æ–­")

            resp.raise_for_status()
            return resp.json()

        except requests.HTTPError as e:
            error_msg = f"HTTP é”™è¯¯: {e.response.status_code}"
            try:
                error_text = e.response.text[:500]
                error_msg += f"\nå“åº”å†…å®¹: {error_text}"
                
                # å°è¯•è§£æ JSON é”™è¯¯
                try:
                    error_json = e.response.json()
                    if 'error' in error_json:
                        error_msg += f"\né”™è¯¯è¯¦æƒ…: {error_json['error']}"
                except:
                    pass
            except Exception as parse_error:
                error_msg += f"\næ— æ³•è§£æå“åº”: {parse_error}"
            raise RuntimeError(error_msg)
            
        except requests.RequestException as e:
            error_msg = f"è¯·æ±‚å¤±è´¥: {type(e).__name__} - {str(e)}"
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_text = e.response.text[:500]
                    error_msg += f"\nå“åº”å†…å®¹: {error_text}"
                except:
                    pass
            raise RuntimeError(error_msg)


class BatchGenerationRunner:
    """æ‰¹é‡ç”Ÿæˆè¿è¡Œå™¨"""

    def __init__(
        self,
        api_client,
        max_workers: int = 4,
        logger_instance=logger,
        interrupt_checker=None
    ):
        self.api_client = api_client
        self.max_workers = max_workers
        self.logger = logger_instance
        self.interrupt_checker = interrupt_checker

    def _ensure_not_interrupted(self):
        if self.interrupt_checker is not None:
            self.interrupt_checker()

    def run_batch(
        self,
        api_key: str,
        request_data_template: Dict[str, Any],
        api_base_url: str,
        batch_size: int,
        timeout: Optional[Tuple[float, float]] = None,
        bypass_proxy: bool = False,
        verify_ssl: bool = True,
    ) -> Tuple[List[Dict[str, Any]], List[str]]:
        """è¿è¡Œæ‰¹é‡ç”Ÿæˆ"""
        self._ensure_not_interrupted()

        results = []
        errors = []

        def _process_batch(batch_num: int, n: int) -> Optional[Dict[str, Any]]:
            """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
            try:
                self._ensure_not_interrupted()

                # åˆ›å»ºè¯¥æ‰¹æ¬¡çš„è¯·æ±‚æ•°æ®
                batch_data = request_data_template.copy()
                batch_data["n"] = n

                # å‘é€è¯·æ±‚
                response_data = self.api_client.send_request(
                    api_key=api_key,
                    request_data=batch_data,
                    api_base_url=api_base_url,
                    timeout=timeout,
                    bypass_proxy=bypass_proxy,
                    verify_ssl=verify_ssl,
                )

                return {
                    "batch": batch_num,
                    "count": n,
                    "response": response_data,
                }

            except Exception as e:
                error_msg = f"æ‰¹æ¬¡ {batch_num} å¤±è´¥: {str(e)}"
                self.logger.warning(error_msg)
                errors.append(error_msg)
                return None

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {}
            remaining = batch_size
            batch_num = 0

            while remaining > 0:
                self._ensure_not_interrupted()
                batch_num += 1
                # æ¯ä¸ªæ‰¹æ¬¡ç”Ÿæˆ 1 å¼ å›¾ç‰‡
                n = 1

                future = executor.submit(_process_batch, batch_num, n)
                futures[future] = batch_num
                remaining -= n

            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                self._ensure_not_interrupted()
                result = future.result()
                if result:
                    results.append(result)

        # æŒ‰æ‰¹æ¬¡å·æ’åº
        results.sort(key=lambda x: x["batch"])
        return results, errors


class ImageGenerationNode:
    """
    é«˜çº§å›¾åƒç”ŸæˆèŠ‚ç‚¹
    æ”¯æŒ Gemini/OpenAI åŒåè®®ã€æ‰¹é‡ç”Ÿæˆã€å¤šå›¾å‚è€ƒ
    """

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("images", "text")
    FUNCTION = "generate"
    OUTPUT_NODE = True
    CATEGORY = "AFOLIE/API/é«˜çº§å›¾åƒèŠ‚ç‚¹"

    # å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆä»… NebulaAI API å®é™…æ”¯æŒçš„æ¨¡å‹ï¼‰
    MODELS = [
        "gemini-3-pro-image-preview",
        "gemini-2.5-flash-image",
    ]

    def __init__(self):
        self.config_manager = ConfigManager(MODULE_DIR)
        self.image_codec = ImageCodec(logger, self._ensure_not_interrupted)
        self.error_canvas = ErrorCanvas(logger)
        self.interrupt_checker = comfy.model_management.throw_exception_if_processing_interrupted

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "prompt": ("STRING", {
                    "multiline": True,
                    "default": "A beautiful sunset over ocean, high quality, detailed",
                    "tooltip": "æ–‡æœ¬æç¤ºè¯ï¼Œæ”¯æŒå¤šè¡Œè¾“å…¥"
                }),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "API å¯†é’¥ï¼ˆå¯é€‰ï¼Œå¯ç”¨ config.ini é…ç½®ï¼‰"
                }),
                "model_type": (cls.MODELS, {
                    "default": "gemini-3-pro-image-preview",
                    "tooltip": "æ¨¡å‹åç§°"
                }),
                "batch_size": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 8,
                    "tooltip": "æ‰¹æ¬¡å¤§å°ï¼ˆ1-8ï¼‰ï¼Œä¸€æ¬¡ç”Ÿæˆå›¾ç‰‡æ•°é‡"
                }),
                "aspect_ratio": (["Auto", "1:1", "16:9", "9:16", "2:3", "3:2", "4:3", "3:4", "4:5", "5:4", "21:9"], {
                    "default": "Auto",
                    "tooltip": "å®½é«˜æ¯”ï¼šAuto/1:1/16:9/9:16/2:3/3:2/4:3/3:4/4:5/5:4/21:9"
                }),
            },
            "optional": {
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 2147483647,
                    "tooltip": "éšæœºç§å­ï¼ˆ-1 ä¸ºéšæœºï¼Œå›ºå®šå€¼å¯å¤ç°ï¼‰"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.95,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "é‡‡æ ·å‚æ•°ï¼ˆ0.0-1.0ï¼‰ï¼Œæ§åˆ¶å¤šæ ·æ€§"
                }),
                "imageSize": (["æ— ", "1K", "2K", "4K"], {
                    "default": "1K",
                    "tooltip": "åˆ†è¾¨ç‡ï¼šæ— /1K/2K/4K"
                }),
                "image_1": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 1"}),
                "image_2": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 2"}),
                "image_3": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 3"}),
                "image_4": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 4"}),
                "image_5": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 5"}),
                "image_6": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 6"}),
                "image_7": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 7"}),
                "image_8": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 8"}),
                "image_9": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 9"}),
                "è¶…æ—¶ç§’æ•°": ("INT", {
                    "default": 1200,
                    "min": 0,
                    "max": 1800,
                    "tooltip": "API è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆ0-1800ç§’ï¼‰"
                }),
                "ç»•è¿‡ä»£ç†": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ¢¯å­ä¸ç¨³å®šæ—¶å¼€å¯"
                }),
                "api_protocol": (["Gemini", "OpenAI"], {
                    "default": "Gemini",
                    "tooltip": "é€‰æ‹© Gemini æˆ– OpenAI åè®®"
                }),
            }
        }

    @staticmethod
    def _ensure_not_interrupted():
        comfy.model_management.throw_exception_if_processing_interrupted()

    def _resolve_api_key(self, provided_key: str) -> str:
        """è§£æ API Key"""
        raw_key = (provided_key or "").strip()
        resolved_key = self.config_manager.sanitize_api_key(raw_key)
        if not resolved_key:
            resolved_key = self.config_manager.sanitize_api_key(
                self.config_manager.load_api_key()
            )

        if not resolved_key:
            raise ValueError("è¯·é…ç½®æœ‰æ•ˆçš„ API Keyï¼ˆåœ¨ config.ini æˆ–èŠ‚ç‚¹è¾“å…¥ä¸­ï¼‰")

        return resolved_key

    def _get_api_base_url(self) -> str:
        """è·å– API Base URLï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰"""
        return self.config_manager.get_effective_api_base_url()

    def _collect_input_images(self, **kwargs) -> List[torch.Tensor]:
        """æ”¶é›†æ‰€æœ‰è¾“å…¥å›¾åƒ"""
        images = []
        for i in range(1, 10):
            img_key = f"image_{i}"
            img = kwargs.get(img_key)
            if img is not None:
                images.append(img)
        return images

    def _extract_images_from_response(
        self,
        response_data: Dict[str, Any],
        protocol: str
    ) -> List[str]:
        """ä»å“åº”ä¸­æå–å›¾ç‰‡"""
        images: List[str] = []

        if "data" in response_data:
            data = response_data["data"]

            # å¤„ç†åµŒå¥—çš„ data ç»“æ„
            if isinstance(data, dict) and "data" in data:
                data = data["data"]

            if isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        # b64_json æ ¼å¼
                        if "b64_json" in item and item["b64_json"]:
                            images.append(item["b64_json"])
                        # url æ ¼å¼
                        elif "url" in item and item["url"]:
                            b64_data = self._download_image_to_base64(item["url"])
                            if b64_data:
                                images.append(b64_data)

        return images

    def _download_image_to_base64(self, url: str, timeout: float = 30.0) -> Optional[str]:
        """ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64"""
        try:
            session = requests.Session()
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "image/*,*/*;q=0.8",
            }
            response = session.get(url, headers=headers, timeout=timeout, verify=True)
            response.raise_for_status()

            image_data = response.content
            base64_data = base64.b64encode(image_data).decode('utf-8')
            logger.info(f"å›¾ç‰‡ä¸‹è½½æˆåŠŸï¼š{len(image_data)} å­—èŠ‚")
            return base64_data
        except Exception as exc:
            logger.warning(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼š{type(exc).__name__}")
            return None

    def generate(
        self,
        prompt: str,
        api_key: str = "",
        api_base_url: str = "",
        model_type: str = "gemini-2.5-flash-image",
        batch_size: int = 1,
        aspect_ratio: str = "Auto",
        seed: int = -1,
        top_p: float = 1.0,
        imageSize: str = "æ— ",
        è¶…æ—¶ç§’æ•°: int = 420,
        ç»•è¿‡ä»£ç†: bool = False,
        api_protocol: str = "Gemini",
        **kwargs
    ):
        """ç”Ÿæˆå›¾åƒ"""
        start_time = time.time()

        try:
            self._ensure_not_interrupted()

            # è§£æ API Key
            resolved_api_key = self._resolve_api_key(api_key)

            # è·å– API Base URLï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
            resolved_api_base_url = self._get_api_base_url()

            # æ”¶é›†è¾“å…¥å›¾åƒ
            input_tensors = self._collect_input_images(**kwargs)
            input_images_b64 = []
            if input_tensors:
                input_images_b64 = self.image_codec.prepare_input_images(input_tensors)

            # è‡ªåŠ¨æ£€æµ‹å®½é«˜æ¯”
            effective_aspect_ratio = aspect_ratio
            if aspect_ratio == "Auto" and input_tensors:
                first_img = input_tensors[0]
                if first_img is not None and len(first_img.shape) >= 3:
                    h, w = first_img.shape[1], first_img.shape[2]
                    ratio = w / h
                    aspect_ratios = {
                        "1:1": 1.0, "3:2": 1.5, "2:3": 0.667, "3:4": 0.75,
                        "4:3": 1.333, "4:5": 0.8, "5:4": 1.25, "9:16": 0.5625,
                        "16:9": 1.778, "21:9": 2.333
                    }
                    closest = min(aspect_ratios.items(), key=lambda x: abs(x[1] - ratio))
                    effective_aspect_ratio = closest[0]
                    logger.info(f"è‡ªåŠ¨æ£€æµ‹å®½é«˜æ¯”: {effective_aspect_ratio} (åŸå§‹æ¯”ä¾‹: {ratio:.2f})")

            # è·å–æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            max_workers = self.config_manager.load_max_workers()

            # æ ¹æ®åè®®é€‰æ‹©å®¢æˆ·ç«¯
            if api_protocol == "Gemini":
                api_client = GeminiApiClient(
                    self.config_manager,
                    logger,
                    self.interrupt_checker
                )
            else:  # OpenAI
                api_client = OpenAIApiClient(
                    self.config_manager,
                    logger,
                    self.interrupt_checker
                )

            # åˆ›å»ºæ‰¹é‡ç”Ÿæˆè¿è¡Œå™¨
            batch_runner = BatchGenerationRunner(
                api_client,
                max_workers=max_workers,
                logger_instance=logger,
                interrupt_checker=self.interrupt_checker
            )

            # å¤„ç†è¶…æ—¶
            timeout_value = None if è¶…æ—¶ç§’æ•° == 0 else (15, è¶…æ—¶ç§’æ•°)

            logger.header("ğŸ¨ é«˜çº§å›¾åƒç”Ÿæˆä»»åŠ¡")
            logger.info(f"åè®®: {api_protocol}")
            logger.info(f"æ¨¡å‹: {model_type}")
            logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
            logger.info(f"å®½é«˜æ¯”: {effective_aspect_ratio}")
            logger.info(f"åˆ†è¾¨ç‡: {imageSize}")
            if input_tensors:
                logger.info(f"å‚è€ƒå›¾åƒ: {len(input_tensors)} å¼ ")
            if seed >= 0:
                logger.info(f"ç§å­: {seed}")
            logger.separator()

            # åˆ›å»ºè¯·æ±‚æ•°æ®æ¨¡æ¿
            if api_protocol == "Gemini":
                # Gemini åè®®éœ€è¦åŒæ—¶ä¼ é€’ sizeï¼ˆå®½é«˜æ¯”ï¼‰å’Œ qualityï¼ˆè´¨é‡æ˜ å°„åˆ°åˆ†è¾¨ç‡ï¼‰
                # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å‚æ•°å size å’Œ quality
                request_data_template = api_client.create_request_data(
                    model=model_type,
                    prompt=prompt,
                    size=effective_aspect_ratio if effective_aspect_ratio != "Auto" else None,
                    quality=imageSize,  # "1K"/"2K"/"4K" ä¼šè‡ªåŠ¨æ˜ å°„
                    input_images_b64=input_images_b64 if input_images_b64 else None,
                    top_p=top_p if top_p != 1.0 else None,
                    seed=seed if seed >= 0 else None,
                )
                
                # å…³é”®ä¿®å¤ï¼šç¡®ä¿ size å’Œ image_size å‚æ•°è¢«æ­£ç¡®ä¼ é€’
                if effective_aspect_ratio != "Auto" and "size" not in request_data_template:
                    request_data_template["size"] = effective_aspect_ratio
                if imageSize != "æ— " and "image_size" not in request_data_template:
                    # æ˜ å°„ imageSize åˆ° image_size
                    request_data_template["image_size"] = imageSize
            else:  # OpenAI
                # OpenAI åè®®éœ€è¦è½¬æ¢å®½é«˜æ¯”ä¸ºå°ºå¯¸
                size_map = {
                    "1:1": "1024x1024",
                    "16:9": "1024x576",
                    "9:16": "576x1024",
                    "2:3": "768x1152",
                    "3:2": "1152x768",
                    "4:3": "1024x768",
                    "3:4": "768x1024",
                    "4:5": "832x1040",
                    "5:4": "1040x832",
                    "21:9": "1344x576",
                    "Auto": "1024x1024",
                }

                size = size_map.get(effective_aspect_ratio, "1024x1024")

                request_data_template = api_client.create_request_data(
                    model=model_type,
                    prompt=prompt,
                    size=size,
                    input_images_b64=input_images_b64 if input_images_b64 else None,
                    top_p=top_p if top_p != 1.0 else None,
                    seed=seed if seed >= 0 else None,
                )

            # æ‰§è¡Œæ‰¹é‡ç”Ÿæˆ
            batch_results, batch_errors = batch_runner.run_batch(
                api_key=resolved_api_key,
                request_data_template=request_data_template,
                api_base_url=resolved_api_base_url,
                batch_size=batch_size,
                timeout=timeout_value,
                bypass_proxy=ç»•è¿‡ä»£ç†,
                verify_ssl=True,
            )

            self._ensure_not_interrupted()

            # æå–æ‰€æœ‰å›¾ç‰‡
            all_images = []
            for result in batch_results:
                images = self._extract_images_from_response(result["response"], api_protocol)
                all_images.extend(images)

            if not all_images:
                error_msg = f"ç”Ÿæˆå¤±è´¥ï¼šæœªè¿”å›ä»»ä½•å›¾ç‰‡"
                if batch_errors:
                    error_msg += f"\né”™è¯¯è¯¦æƒ…ï¼š\n" + "\n".join(batch_errors[:3])

                error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", error_msg)
                return (error_tensor, error_msg)

            # è§£ç å›¾ç‰‡
            self._ensure_not_interrupted()
            image_tensor = self.image_codec.base64_to_tensor_parallel(
                all_images,
                log_prefix="é«˜çº§ç”Ÿæˆ",
                max_workers=max_workers
            )

            total_time = time.time() - start_time
            actual_count = len(all_images)
            avg_time = total_time / actual_count if actual_count > 0 else 0

            # æ„å»ºè¿”å›ä¿¡æ¯
            info_text = f"âœ… æˆåŠŸç”Ÿæˆ {actual_count} å¼ å›¾åƒ\n"
            info_text += f"åè®®: {api_protocol}\n"
            info_text += f"æ¨¡å‹: {model_type}\n"
            info_text += f"å®½é«˜æ¯”: {effective_aspect_ratio}\n"
            if imageSize != "æ— ":
                info_text += f"åˆ†è¾¨ç‡: {imageSize}\n"
            info_text += f"æ‰¹æ¬¡å¤§å°: {batch_size}\n"
            info_text += f"æ€»è€—æ—¶: {total_time:.2f}sï¼Œå¹³å‡ {avg_time:.2f}s/å¼ "

            if batch_errors:
                info_text += f"\n\nâš ï¸ éƒ¨åˆ†è¯·æ±‚å¤±è´¥ ({len(batch_errors)} ä¸ª)"

            # æ˜¾ç¤ºå®Œæˆç»Ÿè®¡
            logger.summary("ä»»åŠ¡å®Œæˆ", {
                "æˆåŠŸç”Ÿæˆ": f"{actual_count} å¼ ",
                "æ€»è€—æ—¶": f"{total_time:.2f}s",
                "å¹³å‡é€Ÿåº¦": f"{avg_time:.2f}s/å¼ "
            })

            return (image_tensor, info_text)

        except comfy.model_management.InterruptProcessingException:
            logger.warning("ä»»åŠ¡å·²å–æ¶ˆ")
            raise
        except Exception as e:
            error_msg = str(e)[:500]
            logger.error(f"ç”Ÿæˆå¤±è´¥: {error_msg}")
            error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", error_msg)
            return (error_tensor, error_msg)


# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "AFOLIE_AdvancedImageGeneration": ImageGenerationNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AFOLIE_AdvancedImageGeneration": "ğŸ¨ AFOLIE é«˜çº§å›¾åƒç”Ÿæˆ",
}
