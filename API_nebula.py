"""
Nebula å›¾ç‰‡ç”ŸæˆèŠ‚ç‚¹
æ”¯æŒ Geminiã€è±†åŒ… Seedreamã€GPT Imageã€é€šä¹‰åƒé—®ç­‰å¤šç§æ¨¡å‹
"""

from __future__ import annotations

import json
import torch
from typing import List, Dict, Optional, Tuple, Any
import time
import os
import sys

MODULE_DIR = os.path.dirname(os.path.abspath(__file__))
if MODULE_DIR not in sys.path:
    sys.path.insert(0, MODULE_DIR)

try:
    from server import PromptServer
except ImportError:
    class _DummyPromptServer:
        instance = None
    PromptServer = _DummyPromptServer()

import comfy.utils
import comfy.model_management

from nebula_logger import logger
from nebula_config_manager import ConfigManager
from nebula_image_codec import ImageCodec, ErrorCanvas
from nebula_api_client import NebulaApiClient


CONFIG_MANAGER = ConfigManager(MODULE_DIR)
API_CLIENT = NebulaApiClient(
    CONFIG_MANAGER,
    logger,
    interrupt_checker=comfy.model_management.throw_exception_if_processing_interrupted,
)


# æ¨¡å‹åˆ—è¡¨
GEMINI_MODELS = [
    "gemini-3-pro-image-preview",
    "gemini-2.5-flash-image",
]

DOUBAO_MODELS = [
    "doubao-seedream-3-0-t2i-250415",
    "doubao-seedream-4-0-250828",
    "doubao-seedream-4-5-251128",
    "doubao-seededit-3-0-i2i-250628",
]

GPT_MODELS = [
    "gpt-image-1",
    "gpt-image-1-mini",
]

QWEN_MODELS = [
    "qwen-image-plus",
    "qwen-image-edit-plus",
]

ALL_MODELS = GEMINI_MODELS + DOUBAO_MODELS + GPT_MODELS + QWEN_MODELS


class NebulaImageGenerator:
    """
    ComfyUIèŠ‚ç‚¹: Nebula å›¾åƒç”Ÿæˆ
    æ”¯æŒå¤šç§ AI å›¾åƒç”Ÿæˆæ¨¡å‹
    """

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("å›¾åƒ", "ä¿¡æ¯")
    FUNCTION = "generate_images"
    OUTPUT_NODE = True
    CATEGORY = "AFOLIE/API/nebulaå›¾åƒèŠ‚ç‚¹"

    def __init__(self):
        self.config_manager = CONFIG_MANAGER
        self.image_codec = ImageCodec(logger, self._ensure_not_interrupted)
        self.error_canvas = ErrorCanvas(logger)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "æç¤ºè¯": ("STRING", {
                    "multiline": True,
                    "default": "ä¸€åªå¯çˆ±çš„æ©™è‰²å°çŒ«ååœ¨èŠ±å›­é‡Œï¼Œé˜³å…‰æ˜åªšï¼Œé«˜è´¨é‡æ‘„å½±",
                    "tooltip": "ç”Ÿæˆå›¾åƒçš„æ–‡æœ¬æç¤ºè¯"
                }),
                "æ¨¡å‹": (ALL_MODELS, {
                    "default": "gemini-2.5-flash-image",
                    "tooltip": "é€‰æ‹©å›¾åƒç”Ÿæˆæ¨¡å‹"
                }),
                "APIå¯†é’¥": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "API Keyï¼Œç•™ç©ºåˆ™ä½¿ç”¨ config.ini ä¸­çš„é…ç½®"
                }),
            },
            "optional": {
                "å›¾ç‰‡å°ºå¯¸": ("STRING", {
                    "default": "1024x1024",
                    "multiline": False,
                    "tooltip": "å›¾ç‰‡å°ºå¯¸ï¼Œå¦‚ 1024x1024ã€16:9ã€2048x2048 ç­‰"
                }),
                "å›¾ç‰‡è´¨é‡": (["auto", "low", "medium", "high", "hd", "1K", "2K", "4K"], {
                    "default": "high",
                    "tooltip": "å›¾ç‰‡è´¨é‡è®¾ç½®"
                }),
                "ç”Ÿæˆæ•°é‡": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 10,
                    "tooltip": "ç”Ÿæˆå›¾ç‰‡æ•°é‡"
                }),
                "å‚è€ƒå›¾åƒ": ("IMAGE", {
                    "tooltip": "å‚è€ƒå›¾åƒï¼Œç”¨äºå›¾ç”Ÿå›¾"
                }),
                "è´Ÿé¢æç¤ºè¯": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "è´Ÿé¢æç¤ºè¯ï¼Œç”¨äºæ’é™¤ä¸æƒ³è¦çš„å…ƒç´ "
                }),
                "éšæœºç§å­": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 2147483647,
                    "tooltip": "éšæœºç§å­ï¼Œ-1 ä¸ºéšæœº"
                }),
                "è¶…æ—¶ç§’æ•°": ("INT", {
                    "default": 420,
                    "min": 30,
                    "max": 1800,
                    "tooltip": "API è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"
                }),
                "æ°´å°": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦æ·»åŠ æ°´å°ï¼ˆè±†åŒ…/é€šä¹‰åƒé—®ï¼‰"
                }),
                "æç¤ºè¯æ‰©å±•": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¯å¦å¯ç”¨æç¤ºè¯æ‰©å±•ï¼ˆé€šä¹‰åƒé—®ï¼‰"
                }),
                "å¼•å¯¼ç³»æ•°": ("FLOAT", {
                    "default": 2.5,
                    "min": 1.0,
                    "max": 10.0,
                    "step": 0.5,
                    "tooltip": "å¼•å¯¼ç³»æ•°ï¼ˆè±†åŒ… Seedream 3.0ï¼‰"
                }),
                "ä¼˜åŒ–æ¨¡å¼": (["standard", "fast", "creative", "precise"], {
                    "default": "standard",
                    "tooltip": "æç¤ºè¯ä¼˜åŒ–æ¨¡å¼ï¼ˆè±†åŒ… Seedream 4.xï¼‰"
                }),
                "è¾“å…¥ä¿çœŸåº¦": (["auto", "low", "medium", "high"], {
                    "default": "medium",
                    "tooltip": "è¾“å…¥å›¾ç‰‡ä¿çœŸåº¦ï¼ˆGPT Imageï¼‰"
                }),
            }
        }

    @staticmethod
    def _ensure_not_interrupted():
        comfy.model_management.throw_exception_if_processing_interrupted()

    def _get_model_category(self, model: str) -> str:
        """è·å–æ¨¡å‹ç±»åˆ«"""
        if model in GEMINI_MODELS:
            return "gemini"
        elif model in DOUBAO_MODELS:
            return "doubao"
        elif model in GPT_MODELS:
            return "gpt"
        elif model in QWEN_MODELS:
            return "qwen"
        return "unknown"

    def _build_extra_params(
        self,
        model: str,
        negative_prompt: str,
        seed: int,
        watermark: bool,
        prompt_extend: bool,
        guidance_scale: float,
        optimize_mode: str,
        input_fidelity: str,
    ) -> Dict[str, Any]:
        """æ ¹æ®æ¨¡å‹ç±»å‹æ„å»ºé¢å¤–å‚æ•°"""
        extra_params = {}
        category = self._get_model_category(model)

        if category == "gemini":
            # Gemini å‚æ•°
            if seed >= 0:
                extra_params["seed"] = seed

        elif category == "doubao":
            # è±†åŒ… Seedream å‚æ•°
            extra_params["watermark"] = watermark
            if seed >= 0:
                extra_params["seed"] = seed
            
            # 3.0 æ¨¡å‹æ”¯æŒå¼•å¯¼ç³»æ•°
            if "3-0" in model:
                extra_params["guidance_scale"] = guidance_scale
            
            # 4.x æ¨¡å‹æ”¯æŒä¼˜åŒ–æ¨¡å¼
            if "4-0" in model or "4-5" in model:
                extra_params["optimize_prompt_options"] = {"mode": optimize_mode}

        elif category == "gpt":
            # GPT Image å‚æ•°
            extra_params["input_fidelity"] = input_fidelity

        elif category == "qwen":
            # é€šä¹‰åƒé—®å‚æ•°
            params = {
                "watermark": watermark,
                "prompt_extend": prompt_extend,
            }
            if negative_prompt:
                params["negative_prompt"] = negative_prompt
            if seed >= 0:
                params["seed"] = seed
            extra_params["parameters"] = params

        return extra_params

    def generate_images(
        self,
        æç¤ºè¯: str,
        æ¨¡å‹: str,
        APIå¯†é’¥: str = "",
        å›¾ç‰‡å°ºå¯¸: str = "1024x1024",
        å›¾ç‰‡è´¨é‡: str = "high",
        ç”Ÿæˆæ•°é‡: int = 1,
        å‚è€ƒå›¾åƒ: Optional[torch.Tensor] = None,
        è´Ÿé¢æç¤ºè¯: str = "",
        éšæœºç§å­: int = -1,
        è¶…æ—¶ç§’æ•°: int = 420,
        æ°´å°: bool = False,
        æç¤ºè¯æ‰©å±•: bool = True,
        å¼•å¯¼ç³»æ•°: float = 2.5,
        ä¼˜åŒ–æ¨¡å¼: str = "standard",
        è¾“å…¥ä¿çœŸåº¦: str = "medium",
    ):
        """ç”Ÿæˆå›¾åƒ"""
        start_time = time.time()

        # è§£æ API Key
        raw_api_key = (APIå¯†é’¥ or "").strip()
        resolved_api_key = self.config_manager.sanitize_api_key(raw_api_key)
        if not resolved_api_key:
            resolved_api_key = self.config_manager.sanitize_api_key(
                self.config_manager.load_api_key()
            )

        if not resolved_api_key:
            error_msg = "è¯·åœ¨ config.ini ä¸­é…ç½® API Key æˆ–åœ¨èŠ‚ç‚¹ä¸­å¡«å†™"
            logger.error(error_msg)
            error_tensor = self.error_canvas.build_error_tensor_from_text(
                "é…ç½®ç¼ºå¤±",
                f"{error_msg}\nè¯·åœ¨ config.ini æˆ–èŠ‚ç‚¹è¾“å…¥ä¸­å¡«å†™æœ‰æ•ˆ API Key"
            )
            return (error_tensor, error_msg)

        # è·å– API Base URL
        api_base_url = self.config_manager.get_effective_api_base_url()

        # è¾“å‡ºé…ç½®ä¿¡æ¯
        masked_key = resolved_api_key[:8] + "..." + resolved_api_key[-4:] if len(resolved_api_key) > 12 else "***"
        logger.info(f"ä½¿ç”¨ API Base URL: {api_base_url}")
        logger.info(f"ä½¿ç”¨ API Key: {masked_key}")
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {æ¨¡å‹}")

        # å‡†å¤‡è¾“å…¥å›¾åƒ
        input_images_b64 = []
        if å‚è€ƒå›¾åƒ is not None:
            input_images_b64 = self.image_codec.prepare_input_images([å‚è€ƒå›¾åƒ])

        # æ„å»ºé¢å¤–å‚æ•°
        extra_params = self._build_extra_params(
            æ¨¡å‹, è´Ÿé¢æç¤ºè¯, éšæœºç§å­, æ°´å°, æç¤ºè¯æ‰©å±•,
            å¼•å¯¼ç³»æ•°, ä¼˜åŒ–æ¨¡å¼, è¾“å…¥ä¿çœŸåº¦
        )

        # æ˜¾ç¤ºä»»åŠ¡ä¿¡æ¯
        logger.header("ğŸŒŒ Nebula å›¾åƒç”Ÿæˆä»»åŠ¡")
        logger.info(f"æ¨¡å‹: {æ¨¡å‹}")
        logger.info(f"å°ºå¯¸: {å›¾ç‰‡å°ºå¯¸}")
        logger.info(f"è´¨é‡: {å›¾ç‰‡è´¨é‡}")
        logger.info(f"æ•°é‡: {ç”Ÿæˆæ•°é‡}")
        if éšæœºç§å­ >= 0:
            logger.info(f"ç§å­: {éšæœºç§å­}")
        logger.separator()

        try:
            self._ensure_not_interrupted()

            # åˆ›å»ºè¯·æ±‚æ•°æ®
            request_data = API_CLIENT.create_request_data(
                model=æ¨¡å‹,
                prompt=æç¤ºè¯,
                size=å›¾ç‰‡å°ºå¯¸,
                quality=å›¾ç‰‡è´¨é‡,
                n=ç”Ÿæˆæ•°é‡,
                response_format="b64_json",
                input_images_b64=input_images_b64 if input_images_b64 else None,
                **extra_params
            )

            self._ensure_not_interrupted()

            # å‘é€è¯·æ±‚
            response_data = API_CLIENT.send_request(
                resolved_api_key,
                request_data,
                api_base_url,
                timeout=(15, è¶…æ—¶ç§’æ•°),
                bypass_proxy=False,
                verify_ssl=True,
            )

            self._ensure_not_interrupted()

            # æå–å›¾ç‰‡
            base64_images, revised_prompt = API_CLIENT.extract_images(response_data)

            if not base64_images:
                error_msg = "API æœªè¿”å›ä»»ä½•å›¾ç‰‡"
                logger.warning(error_msg)
                error_tensor = self.error_canvas.build_error_tensor_from_text(
                    "ç”Ÿæˆå¤±è´¥", error_msg
                )
                return (error_tensor, error_msg)

            # è§£ç å›¾ç‰‡
            self._ensure_not_interrupted()
            image_tensor = self.image_codec.base64_to_tensor_parallel(
                base64_images,
                log_prefix="Nebula",
                max_workers=self.config_manager.load_max_workers()
            )

            total_time = time.time() - start_time
            actual_count = len(base64_images)
            avg_time = total_time / actual_count if actual_count > 0 else 0

            # æ„å»ºè¿”å›ä¿¡æ¯
            info_text = f"âœ… æˆåŠŸç”Ÿæˆ {actual_count} å¼ å›¾åƒ\n"
            info_text += f"æ¨¡å‹: {æ¨¡å‹}\n"
            info_text += f"å°ºå¯¸: {å›¾ç‰‡å°ºå¯¸}\n"
            info_text += f"æ€»è€—æ—¶: {total_time:.2f}sï¼Œå¹³å‡ {avg_time:.2f}s/å¼ "
            if revised_prompt:
                info_text += f"\nä¿®è®¢æç¤ºè¯: {revised_prompt}"

            # æ˜¾ç¤ºå®Œæˆç»Ÿè®¡
            logger.summary("ä»»åŠ¡å®Œæˆ", {
                "ç”Ÿæˆæ•°é‡": f"{actual_count} å¼ ",
                "æ€»è€—æ—¶": f"{total_time:.2f}s",
                "å¹³å‡é€Ÿåº¦": f"{avg_time:.2f}s/å¼ "
            })

            return (image_tensor, info_text)

        except comfy.model_management.InterruptProcessingException:
            logger.warning("ä»»åŠ¡å·²å–æ¶ˆ")
            raise
        except Exception as e:
            error_msg = str(e)[:500]
            logger.error(f"ç”Ÿæˆå¤±è´¥: {error_msg}")
            error_tensor = self.error_canvas.build_error_tensor_from_text(
                "ç”Ÿæˆå¤±è´¥", error_msg
            )
            return (error_tensor, error_msg)


class NebulaGeminiNode:
    """
    Gemini å›¾åƒç”ŸæˆèŠ‚ç‚¹
    ä¸“é—¨é’ˆå¯¹ Gemini æ¨¡å‹ä¼˜åŒ–çš„èŠ‚ç‚¹
    æ”¯æŒåŠ¨æ€å‚è€ƒå›¾åƒè¾“å…¥ç«¯å£
    """

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("å›¾åƒ", "ä¿¡æ¯")
    FUNCTION = "generate"
    OUTPUT_NODE = True
    CATEGORY = "AFOLIE/API/nebulaå›¾åƒèŠ‚ç‚¹"

    def __init__(self):
        self.config_manager = CONFIG_MANAGER
        self.image_codec = ImageCodec(logger, self._ensure_not_interrupted)
        self.error_canvas = ErrorCanvas(logger)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "æç¤ºè¯": ("STRING", {
                    "multiline": True,
                    "default": "ä¸€åªå¯çˆ±çš„æ©™è‰²å°çŒ«ååœ¨èŠ±å›­é‡Œï¼Œé˜³å…‰æ˜åªšï¼Œé«˜è´¨é‡æ‘„å½±",
                }),
                "æ¨¡å‹": (GEMINI_MODELS, {
                    "default": "gemini-3-pro-image-preview",
                }),
                "APIå¯†é’¥": ("STRING", {
                    "default": "",
                    "multiline": False,
                }),
            },
            "optional": {
                "å®½é«˜æ¯”": (["Auto", "1:1", "3:2", "2:3", "3:4", "4:3", "4:5", "5:4", "9:16", "16:9", "21:9"], {
                    "default": "Auto",
                    "tooltip": "Auto ä¼šè‡ªåŠ¨åŒ¹é…å‚è€ƒå›¾åƒçš„ç›¸è¿‘å®½é«˜æ¯”"
                }),
                "å›¾ç‰‡å°ºå¯¸": (["1K", "2K", "4K"], {
                    "default": "2K",
                    "tooltip": "è¾“å‡ºå›¾ç‰‡åˆ†è¾¨ç‡ï¼š1K/2K/4K"
                }),
                "ç”Ÿæˆæ•°é‡": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 4,
                }),
                "è¶…æ—¶ç§’æ•°": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 1800,
                    "tooltip": "API è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œ0 è¡¨ç¤ºæ— é™ç­‰å¾…"
                }),
                "å‚è€ƒå›¾åƒ1": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 1"}),
                "å‚è€ƒå›¾åƒ2": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 2"}),
                "å‚è€ƒå›¾åƒ3": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 3"}),
                "å‚è€ƒå›¾åƒ4": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 4"}),
                "å‚è€ƒå›¾åƒ5": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 5"}),
                "å‚è€ƒå›¾åƒ6": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 6"}),
                "å‚è€ƒå›¾åƒ7": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 7"}),
                "å‚è€ƒå›¾åƒ8": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 8"}),
                "å‚è€ƒå›¾åƒ9": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 9"}),
                "å‚è€ƒå›¾åƒ10": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 10"}),
                "å‚è€ƒå›¾åƒ11": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 11"}),
                "å‚è€ƒå›¾åƒ12": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 12"}),
                "å‚è€ƒå›¾åƒ13": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 13"}),
                "å‚è€ƒå›¾åƒ14": ("IMAGE", {"tooltip": "å‚è€ƒå›¾åƒ 14"}),
            }
        }

    @staticmethod
    def _ensure_not_interrupted():
        comfy.model_management.throw_exception_if_processing_interrupted()

    def generate(
        self,
        æç¤ºè¯: str,
        æ¨¡å‹: str,
        APIå¯†é’¥: str = "",
        å®½é«˜æ¯”: str = "Auto",
        å›¾ç‰‡å°ºå¯¸: str = "2K",
        ç”Ÿæˆæ•°é‡: int = 1,
        è¶…æ—¶ç§’æ•°: int = 0,
        **kwargs
    ):
        """ç”Ÿæˆ Gemini å›¾åƒ"""
        start_time = time.time()

        # è§£æ API Key
        raw_api_key = (APIå¯†é’¥ or "").strip()
        resolved_api_key = self.config_manager.sanitize_api_key(raw_api_key)
        if not resolved_api_key:
            resolved_api_key = self.config_manager.sanitize_api_key(
                self.config_manager.load_api_key()
            )

        if not resolved_api_key:
            error_msg = "è¯·é…ç½® API Key"
            error_tensor = self.error_canvas.build_error_tensor_from_text("é…ç½®ç¼ºå¤±", error_msg)
            return (error_tensor, error_msg)

        api_base_url = self.config_manager.get_effective_api_base_url()

        # æ”¶é›†æ‰€æœ‰å‚è€ƒå›¾åƒ
        input_tensors = []
        for i in range(1, 15):
            img = kwargs.get(f"å‚è€ƒå›¾åƒ{i}")
            if img is not None:
                input_tensors.append(img)
        
        input_images_b64 = self.image_codec.prepare_input_images(input_tensors) if input_tensors else []
        
        # è‡ªåŠ¨æ£€æµ‹å®½é«˜æ¯”
        effective_aspect_ratio = å®½é«˜æ¯”
        if å®½é«˜æ¯” == "Auto" and input_tensors:
            # ä»ç¬¬ä¸€å¼ å‚è€ƒå›¾åƒè·å–å®½é«˜æ¯”
            first_img = input_tensors[0]
            if first_img is not None and len(first_img.shape) >= 3:
                h, w = first_img.shape[1], first_img.shape[2]
                ratio = w / h
                # åŒ¹é…æœ€æ¥è¿‘çš„å®½é«˜æ¯”
                aspect_ratios = {
                    "1:1": 1.0, "3:2": 1.5, "2:3": 0.667, "3:4": 0.75, "4:3": 1.333,
                    "4:5": 0.8, "5:4": 1.25, "9:16": 0.5625, "16:9": 1.778, "21:9": 2.333
                }
                closest = min(aspect_ratios.items(), key=lambda x: abs(x[1] - ratio))
                effective_aspect_ratio = closest[0]
                logger.info(f"è‡ªåŠ¨æ£€æµ‹å®½é«˜æ¯”: {effective_aspect_ratio} (åŸå§‹æ¯”ä¾‹: {ratio:.2f})")

        logger.header("ğŸŒŒ Gemini å›¾åƒç”Ÿæˆ")
        logger.info(f"æ¨¡å‹: {æ¨¡å‹}")
        logger.info(f"å®½é«˜æ¯”: {effective_aspect_ratio}")
        logger.info(f"å›¾ç‰‡å°ºå¯¸: {å›¾ç‰‡å°ºå¯¸}")
        if input_tensors:
            logger.info(f"å‚è€ƒå›¾åƒ: {len(input_tensors)} å¼ ")

        # å¤„ç†è¶…æ—¶ï¼š0 è¡¨ç¤ºæ— é™ç­‰å¾…
        timeout_value = None if è¶…æ—¶ç§’æ•° == 0 else è¶…æ—¶ç§’æ•°

        try:
            self._ensure_not_interrupted()

            # æ„å»ºè¯·æ±‚æ•°æ®ï¼Œä½¿ç”¨ image_size å‚æ•°è®¾ç½®åˆ†è¾¨ç‡
            request_data = API_CLIENT.create_request_data(
                model=æ¨¡å‹,
                prompt=æç¤ºè¯,
                size=effective_aspect_ratio if effective_aspect_ratio != "Auto" else None,
                n=ç”Ÿæˆæ•°é‡,
                response_format="b64_json",
                input_images_b64=input_images_b64 if input_images_b64 else None,
                image_size=å›¾ç‰‡å°ºå¯¸,  # ä½¿ç”¨ image_size å‚æ•°è®¾ç½® 1K/2K/4K
            )

            response_data = API_CLIENT.send_request(
                resolved_api_key,
                request_data,
                api_base_url,
                timeout=(15, timeout_value),
            )

            base64_images, revised_prompt = API_CLIENT.extract_images(response_data)

            if not base64_images:
                error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", "æœªè¿”å›å›¾ç‰‡")
                return (error_tensor, "æœªè¿”å›å›¾ç‰‡")

            image_tensor = self.image_codec.base64_to_tensor_parallel(base64_images)

            total_time = time.time() - start_time
            info_text = f"âœ… ç”Ÿæˆ {len(base64_images)} å¼ å›¾åƒ ({å›¾ç‰‡å°ºå¯¸})ï¼Œè€—æ—¶ {total_time:.2f}s"

            logger.success(info_text)
            return (image_tensor, info_text)

        except Exception as e:
            error_msg = str(e)[:300]
            logger.error(f"ç”Ÿæˆå¤±è´¥: {error_msg}")
            error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", error_msg)
            return (error_tensor, error_msg)


class NebulaDoubaoNode:
    """
    è±†åŒ… Seedream å›¾åƒç”ŸæˆèŠ‚ç‚¹
    """

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("å›¾åƒ", "ä¿¡æ¯")
    FUNCTION = "generate"
    OUTPUT_NODE = True
    CATEGORY = "AFOLIE/API/nebulaå›¾åƒèŠ‚ç‚¹"

    def __init__(self):
        self.config_manager = CONFIG_MANAGER
        self.image_codec = ImageCodec(logger, self._ensure_not_interrupted)
        self.error_canvas = ErrorCanvas(logger)

    @classmethod
    def INPUT_TYPES(cls):
        # è±†åŒ…æ”¯æŒçš„å°ºå¯¸
        sizes_3_0 = ["1024x1024", "1152x864", "864x1152", "1280x720", "720x1280", "1248x832", "832x1248", "1512x648"]
        sizes_4_x = ["2048x2048", "2304x1728", "1728x2304", "2560x1440", "1440x2560", "2496x1664", "1664x2496", "3024x1296"]
        all_sizes = sizes_3_0 + sizes_4_x

        return {
            "required": {
                "æç¤ºè¯": ("STRING", {
                    "multiline": True,
                    "default": "ä¸€åªå¯çˆ±çš„æ©™è‰²å°çŒ«ååœ¨èŠ±å›­é‡Œï¼Œé˜³å…‰æ˜åªšï¼Œé«˜è´¨é‡æ‘„å½±",
                }),
                "æ¨¡å‹": (DOUBAO_MODELS, {
                    "default": "doubao-seedream-4-0-250828",
                }),
                "APIå¯†é’¥": ("STRING", {
                    "default": "",
                    "multiline": False,
                }),
            },
            "optional": {
                "å›¾ç‰‡å°ºå¯¸": (all_sizes, {
                    "default": "2048x2048",
                }),
                "æ°´å°": ("BOOLEAN", {
                    "default": False,
                }),
                "éšæœºç§å­": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 2147483647,
                }),
                "å¼•å¯¼ç³»æ•°": ("FLOAT", {
                    "default": 2.5,
                    "min": 1.0,
                    "max": 10.0,
                    "step": 0.5,
                    "tooltip": "ä»… 3.0 æ¨¡å‹æ”¯æŒ"
                }),
                "ä¼˜åŒ–æ¨¡å¼": (["standard", "fast", "creative", "precise"], {
                    "default": "standard",
                    "tooltip": "4.0 æ”¯æŒ standard/fastï¼Œ4.5 æ”¯æŒ standard/creative/precise"
                }),
                "å‚è€ƒå›¾åƒ": ("IMAGE", {}),
                "è¶…æ—¶ç§’æ•°": ("INT", {
                    "default": 420,
                    "min": 30,
                    "max": 1800,
                }),
            }
        }

    @staticmethod
    def _ensure_not_interrupted():
        comfy.model_management.throw_exception_if_processing_interrupted()

    def generate(
        self,
        æç¤ºè¯: str,
        æ¨¡å‹: str,
        APIå¯†é’¥: str = "",
        å›¾ç‰‡å°ºå¯¸: str = "2048x2048",
        æ°´å°: bool = False,
        éšæœºç§å­: int = -1,
        å¼•å¯¼ç³»æ•°: float = 2.5,
        ä¼˜åŒ–æ¨¡å¼: str = "standard",
        å‚è€ƒå›¾åƒ: Optional[torch.Tensor] = None,
        è¶…æ—¶ç§’æ•°: int = 420,
    ):
        """ç”Ÿæˆè±†åŒ…å›¾åƒ"""
        start_time = time.time()

        raw_api_key = (APIå¯†é’¥ or "").strip()
        resolved_api_key = self.config_manager.sanitize_api_key(raw_api_key)
        if not resolved_api_key:
            resolved_api_key = self.config_manager.sanitize_api_key(
                self.config_manager.load_api_key()
            )

        if not resolved_api_key:
            error_msg = "è¯·é…ç½® API Key"
            error_tensor = self.error_canvas.build_error_tensor_from_text("é…ç½®ç¼ºå¤±", error_msg)
            return (error_tensor, error_msg)

        api_base_url = self.config_manager.get_effective_api_base_url()

        input_images_b64 = []
        if å‚è€ƒå›¾åƒ is not None:
            input_images_b64 = self.image_codec.prepare_input_images([å‚è€ƒå›¾åƒ])

        # æ„å»ºé¢å¤–å‚æ•°
        extra_params = {"watermark": æ°´å°}
        if éšæœºç§å­ >= 0:
            extra_params["seed"] = éšæœºç§å­
        if "3-0" in æ¨¡å‹:
            extra_params["guidance_scale"] = å¼•å¯¼ç³»æ•°
        if "4-0" in æ¨¡å‹ or "4-5" in æ¨¡å‹:
            extra_params["optimize_prompt_options"] = {"mode": ä¼˜åŒ–æ¨¡å¼}

        logger.header("ğŸŒŒ è±†åŒ… Seedream å›¾åƒç”Ÿæˆ")
        logger.info(f"æ¨¡å‹: {æ¨¡å‹}")
        logger.info(f"å°ºå¯¸: {å›¾ç‰‡å°ºå¯¸}")

        try:
            self._ensure_not_interrupted()

            request_data = API_CLIENT.create_request_data(
                model=æ¨¡å‹,
                prompt=æç¤ºè¯,
                size=å›¾ç‰‡å°ºå¯¸,
                n=1,
                response_format="url",
                input_images_b64=input_images_b64 if input_images_b64 else None,
                **extra_params
            )

            response_data = API_CLIENT.send_request(
                resolved_api_key,
                request_data,
                api_base_url,
                timeout=(15, è¶…æ—¶ç§’æ•°),
            )

            base64_images, _ = API_CLIENT.extract_images(response_data)

            if not base64_images:
                error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", "æœªè¿”å›å›¾ç‰‡")
                return (error_tensor, "æœªè¿”å›å›¾ç‰‡")

            image_tensor = self.image_codec.base64_to_tensor_parallel(base64_images)

            total_time = time.time() - start_time
            info_text = f"âœ… ç”Ÿæˆ {len(base64_images)} å¼ å›¾åƒï¼Œè€—æ—¶ {total_time:.2f}s"

            logger.success(info_text)
            return (image_tensor, info_text)

        except Exception as e:
            error_msg = str(e)[:300]
            logger.error(f"ç”Ÿæˆå¤±è´¥: {error_msg}")
            error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", error_msg)
            return (error_tensor, error_msg)


class NebulaGPTImageNode:
    """
    GPT Image å›¾åƒç”ŸæˆèŠ‚ç‚¹
    """

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("å›¾åƒ", "ä¿¡æ¯")
    FUNCTION = "generate"
    OUTPUT_NODE = True
    CATEGORY = "AFOLIE/API/nebulaå›¾åƒèŠ‚ç‚¹"

    def __init__(self):
        self.config_manager = CONFIG_MANAGER
        self.image_codec = ImageCodec(logger, self._ensure_not_interrupted)
        self.error_canvas = ErrorCanvas(logger)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "æç¤ºè¯": ("STRING", {
                    "multiline": True,
                    "default": "ä¸€åªå¯çˆ±çš„æ©™è‰²å°çŒ«ååœ¨èŠ±å›­é‡Œï¼Œé˜³å…‰æ˜åªšï¼Œé«˜è´¨é‡æ‘„å½±",
                }),
                "æ¨¡å‹": (GPT_MODELS, {
                    "default": "gpt-image-1",
                }),
                "APIå¯†é’¥": ("STRING", {
                    "default": "",
                    "multiline": False,
                }),
            },
            "optional": {
                "å›¾ç‰‡å°ºå¯¸": (["1024x1024", "1024x1536", "1536x1024"], {
                    "default": "1024x1024",
                }),
                "å›¾ç‰‡è´¨é‡": (["low", "medium", "high"], {
                    "default": "high",
                }),
                "ç”Ÿæˆæ•°é‡": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 10,
                }),
                "è¾“å…¥ä¿çœŸåº¦": (["auto", "low", "medium", "high"], {
                    "default": "medium",
                }),
                "å‚è€ƒå›¾åƒ": ("IMAGE", {}),
                "è¶…æ—¶ç§’æ•°": ("INT", {
                    "default": 420,
                    "min": 30,
                    "max": 1800,
                }),
            }
        }

    @staticmethod
    def _ensure_not_interrupted():
        comfy.model_management.throw_exception_if_processing_interrupted()

    def generate(
        self,
        æç¤ºè¯: str,
        æ¨¡å‹: str,
        APIå¯†é’¥: str = "",
        å›¾ç‰‡å°ºå¯¸: str = "1024x1024",
        å›¾ç‰‡è´¨é‡: str = "high",
        ç”Ÿæˆæ•°é‡: int = 1,
        è¾“å…¥ä¿çœŸåº¦: str = "medium",
        å‚è€ƒå›¾åƒ: Optional[torch.Tensor] = None,
        è¶…æ—¶ç§’æ•°: int = 420,
    ):
        """ç”Ÿæˆ GPT å›¾åƒ"""
        start_time = time.time()

        raw_api_key = (APIå¯†é’¥ or "").strip()
        resolved_api_key = self.config_manager.sanitize_api_key(raw_api_key)
        if not resolved_api_key:
            resolved_api_key = self.config_manager.sanitize_api_key(
                self.config_manager.load_api_key()
            )

        if not resolved_api_key:
            error_msg = "è¯·é…ç½® API Key"
            error_tensor = self.error_canvas.build_error_tensor_from_text("é…ç½®ç¼ºå¤±", error_msg)
            return (error_tensor, error_msg)

        api_base_url = self.config_manager.get_effective_api_base_url()

        input_images_b64 = []
        if å‚è€ƒå›¾åƒ is not None:
            input_images_b64 = self.image_codec.prepare_input_images([å‚è€ƒå›¾åƒ])

        extra_params = {"input_fidelity": è¾“å…¥ä¿çœŸåº¦}

        logger.header("ğŸŒŒ GPT Image å›¾åƒç”Ÿæˆ")
        logger.info(f"æ¨¡å‹: {æ¨¡å‹}")
        logger.info(f"å°ºå¯¸: {å›¾ç‰‡å°ºå¯¸}")
        logger.info(f"è´¨é‡: {å›¾ç‰‡è´¨é‡}")

        try:
            self._ensure_not_interrupted()

            request_data = API_CLIENT.create_request_data(
                model=æ¨¡å‹,
                prompt=æç¤ºè¯,
                size=å›¾ç‰‡å°ºå¯¸,
                quality=å›¾ç‰‡è´¨é‡,
                n=ç”Ÿæˆæ•°é‡,
                response_format="b64_json",
                input_images_b64=input_images_b64 if input_images_b64 else None,
                **extra_params
            )

            response_data = API_CLIENT.send_request(
                resolved_api_key,
                request_data,
                api_base_url,
                timeout=(15, è¶…æ—¶ç§’æ•°),
            )

            base64_images, _ = API_CLIENT.extract_images(response_data)

            if not base64_images:
                error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", "æœªè¿”å›å›¾ç‰‡")
                return (error_tensor, "æœªè¿”å›å›¾ç‰‡")

            image_tensor = self.image_codec.base64_to_tensor_parallel(base64_images)

            total_time = time.time() - start_time
            info_text = f"âœ… ç”Ÿæˆ {len(base64_images)} å¼ å›¾åƒï¼Œè€—æ—¶ {total_time:.2f}s"

            logger.success(info_text)
            return (image_tensor, info_text)

        except Exception as e:
            error_msg = str(e)[:300]
            logger.error(f"ç”Ÿæˆå¤±è´¥: {error_msg}")
            error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", error_msg)
            return (error_tensor, error_msg)


class NebulaQwenImageNode:
    """
    é€šä¹‰åƒé—®å›¾åƒç”ŸæˆèŠ‚ç‚¹
    """

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("å›¾åƒ", "ä¿¡æ¯")
    FUNCTION = "generate"
    OUTPUT_NODE = True
    CATEGORY = "AFOLIE/API/nebulaå›¾åƒèŠ‚ç‚¹"

    def __init__(self):
        self.config_manager = CONFIG_MANAGER
        self.image_codec = ImageCodec(logger, self._ensure_not_interrupted)
        self.error_canvas = ErrorCanvas(logger)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "æç¤ºè¯": ("STRING", {
                    "multiline": True,
                    "default": "ä¸€åªå¯çˆ±çš„æ©™è‰²å°çŒ«ååœ¨èŠ±å›­é‡Œï¼Œé˜³å…‰æ˜åªšï¼Œé«˜è´¨é‡æ‘„å½±",
                }),
                "æ¨¡å‹": (QWEN_MODELS, {
                    "default": "qwen-image-plus",
                }),
                "APIå¯†é’¥": ("STRING", {
                    "default": "",
                    "multiline": False,
                }),
            },
            "optional": {
                "å›¾ç‰‡å°ºå¯¸": (["1328*1328", "1664*928", "928*1664", "1472*1140", "1140*1472"], {
                    "default": "1328*1328",
                }),
                "è´Ÿé¢æç¤ºè¯": ("STRING", {
                    "default": "",
                    "multiline": True,
                }),
                "æç¤ºè¯æ‰©å±•": ("BOOLEAN", {
                    "default": True,
                }),
                "æ°´å°": ("BOOLEAN", {
                    "default": False,
                }),
                "å‚è€ƒå›¾åƒ": ("IMAGE", {
                    "tooltip": "ä»… qwen-image-edit-plus æ”¯æŒ"
                }),
                "è¶…æ—¶ç§’æ•°": ("INT", {
                    "default": 420,
                    "min": 30,
                    "max": 1800,
                }),
            }
        }

    @staticmethod
    def _ensure_not_interrupted():
        comfy.model_management.throw_exception_if_processing_interrupted()

    def generate(
        self,
        æç¤ºè¯: str,
        æ¨¡å‹: str,
        APIå¯†é’¥: str = "",
        å›¾ç‰‡å°ºå¯¸: str = "1328*1328",
        è´Ÿé¢æç¤ºè¯: str = "",
        æç¤ºè¯æ‰©å±•: bool = True,
        æ°´å°: bool = False,
        å‚è€ƒå›¾åƒ: Optional[torch.Tensor] = None,
        è¶…æ—¶ç§’æ•°: int = 420,
    ):
        """ç”Ÿæˆé€šä¹‰åƒé—®å›¾åƒ"""
        start_time = time.time()

        raw_api_key = (APIå¯†é’¥ or "").strip()
        resolved_api_key = self.config_manager.sanitize_api_key(raw_api_key)
        if not resolved_api_key:
            resolved_api_key = self.config_manager.sanitize_api_key(
                self.config_manager.load_api_key()
            )

        if not resolved_api_key:
            error_msg = "è¯·é…ç½® API Key"
            error_tensor = self.error_canvas.build_error_tensor_from_text("é…ç½®ç¼ºå¤±", error_msg)
            return (error_tensor, error_msg)

        api_base_url = self.config_manager.get_effective_api_base_url()

        input_images_b64 = []
        if å‚è€ƒå›¾åƒ is not None and "edit" in æ¨¡å‹:
            input_images_b64 = self.image_codec.prepare_input_images([å‚è€ƒå›¾åƒ])

        # é€šä¹‰åƒé—®ç‰¹æ®Šå‚æ•°æ ¼å¼
        parameters = {
            "size": å›¾ç‰‡å°ºå¯¸,
            "prompt_extend": æç¤ºè¯æ‰©å±•,
            "watermark": æ°´å°,
        }
        if è´Ÿé¢æç¤ºè¯:
            parameters["negative_prompt"] = è´Ÿé¢æç¤ºè¯

        logger.header("ğŸŒŒ é€šä¹‰åƒé—®å›¾åƒç”Ÿæˆ")
        logger.info(f"æ¨¡å‹: {æ¨¡å‹}")
        logger.info(f"å°ºå¯¸: {å›¾ç‰‡å°ºå¯¸}")

        try:
            self._ensure_not_interrupted()

            request_data = API_CLIENT.create_request_data(
                model=æ¨¡å‹,
                prompt=æç¤ºè¯,
                n=1,
                response_format="b64_json",
                input_images_b64=input_images_b64 if input_images_b64 else None,
                parameters=parameters
            )

            response_data = API_CLIENT.send_request(
                resolved_api_key,
                request_data,
                api_base_url,
                timeout=(15, è¶…æ—¶ç§’æ•°),
            )

            base64_images, _ = API_CLIENT.extract_images(response_data)

            if not base64_images:
                error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", "æœªè¿”å›å›¾ç‰‡")
                return (error_tensor, "æœªè¿”å›å›¾ç‰‡")

            image_tensor = self.image_codec.base64_to_tensor_parallel(base64_images)

            total_time = time.time() - start_time
            info_text = f"âœ… ç”Ÿæˆ {len(base64_images)} å¼ å›¾åƒï¼Œè€—æ—¶ {total_time:.2f}s"

            logger.success(info_text)
            return (image_tensor, info_text)

        except Exception as e:
            error_msg = str(e)[:300]
            logger.error(f"ç”Ÿæˆå¤±è´¥: {error_msg}")
            error_tensor = self.error_canvas.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", error_msg)
            return (error_tensor, error_msg)


# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "AFOLIE_NebulaImageGenerator": NebulaImageGenerator,
    "AFOLIE_NebulaGemini": NebulaGeminiNode,
    "AFOLIE_NebulaDoubao": NebulaDoubaoNode,
    "AFOLIE_NebulaGPTImage": NebulaGPTImageNode,
    "AFOLIE_NebulaQwenImage": NebulaQwenImageNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AFOLIE_NebulaImageGenerator": "ğŸŒŒ Nebula å›¾åƒç”Ÿæˆ",
    "AFOLIE_NebulaGemini": "ğŸŒŒ Nebula Gemini",
    "AFOLIE_NebulaDoubao": "ğŸŒŒ Nebula è±†åŒ… Seedream",
    "AFOLIE_NebulaGPTImage": "ğŸŒŒ Nebula GPT Image",
    "AFOLIE_NebulaQwenImage": "ğŸŒŒ Nebula é€šä¹‰åƒé—®",
}
